{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1eEdH8Hhyy16t0L4SGoAOb0QfA4kf4bso",
      "authorship_tag": "ABX9TyPcBkdzlCfa5cVYzxdFznOO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jspark0914/machine_leraning/blob/master/%EA%B3%BC%EC%A0%9C1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LMqwvrYEAxkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train.csv 파일 활용"
      ],
      "metadata": {
        "id": "J8GiWmQwA2QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "titanic = pd.read_csv(\"/content/drive/MyDrive/machine_dataset/train.csv\")\n",
        "print(titanic)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZa2rba9AQbN",
        "outputId": "fad69c48-4841-452c-daa0-68c692448a77"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     PassengerId  Survived  Pclass  \\\n",
            "0              1         0       3   \n",
            "1              2         1       1   \n",
            "2              3         1       3   \n",
            "3              4         1       1   \n",
            "4              5         0       3   \n",
            "..           ...       ...     ...   \n",
            "886          887         0       2   \n",
            "887          888         1       1   \n",
            "888          889         0       3   \n",
            "889          890         1       1   \n",
            "890          891         0       3   \n",
            "\n",
            "                                                  Name     Sex   Age  SibSp  \\\n",
            "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                             Allen, Mr. William Henry    male  35.0      0   \n",
            "..                                                 ...     ...   ...    ...   \n",
            "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
            "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
            "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
            "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
            "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
            "\n",
            "     Parch            Ticket     Fare Cabin Embarked  \n",
            "0        0         A/5 21171   7.2500   NaN        S  \n",
            "1        0          PC 17599  71.2833   C85        C  \n",
            "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3        0            113803  53.1000  C123        S  \n",
            "4        0            373450   8.0500   NaN        S  \n",
            "..     ...               ...      ...   ...      ...  \n",
            "886      0            211536  13.0000   NaN        S  \n",
            "887      0            112053  30.0000   B42        S  \n",
            "888      2        W./C. 6607  23.4500   NaN        S  \n",
            "889      0            111369  30.0000  C148        C  \n",
            "890      0            370376   7.7500   NaN        Q  \n",
            "\n",
            "[891 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nan_values = titanic.isnull().sum()\n",
        "\n",
        "print(nan_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFItpQBmAmrq",
        "outputId": "9f906ad1-13c2-4802-b729-f22984ca995f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PassengerId      0\n",
            "Survived         0\n",
            "Pclass           0\n",
            "Name             0\n",
            "Sex              0\n",
            "Age            177\n",
            "SibSp            0\n",
            "Parch            0\n",
            "Ticket           0\n",
            "Fare             0\n",
            "Cabin          687\n",
            "Embarked         2\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic['Age'].fillna(titanic['Age'].mean(), inplace=True)\n",
        "titanic['Cabin'].fillna('Unknown', inplace=True)\n",
        "most_frequent = titanic['Embarked'].mode()[0]\n",
        "titanic['Embarked'].fillna(most_frequent, inplace=True)"
      ],
      "metadata": {
        "id": "QwFrnHMSqH3-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드는 titanic 데이터셋의 누락된 값을 처리합니다. 'Age' 열의 결측값은 평균 나이로, 'Cabin'은 'Unknown'으로 채워지며, 'Embarked' 열의 결측값은 가장 빈번한 승선 항구로 대체됩니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "AgtaUHy1EMuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xuv2jEJkD3GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "X = titanic.drop(\"Survived\", axis=1)\n",
        "y = titanic[\"Survived\"]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=202035157)\n",
        "\n",
        "\n",
        "num_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "cat_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "\n",
        "num_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', num_transformer, num_features),\n",
        "        ('cat', cat_transformer, cat_features)])\n",
        "\n",
        "\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n"
      ],
      "metadata": {
        "id": "8QfNCQTEccRN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드는 titanic 데이터셋을 전처리하기 위해 sklearn 라이브러리를 활용합니다. 먼저, 데이터셋을 특성과 레이블로 분리하고, 훈련 세트와 테스트 세트로 나눕니다. 그 후, 수치형과 범주형 특성을 각각 처리하는 파이프라인을 만들고, 이를 ColumnTransformer에 적용하여 전처리된 데이터를 얻습니다. 이렇게 전처리된 데이터는 모델 학습에 사용됩니다. 수업시간에 train,validation,test 데이터 셋 비율을 6:2:2 로 많이 한다고 배웠기에 train과 test 데이터 셋 비율을 8:2로 하였습니다."
      ],
      "metadata": {
        "id": "3UEYa8RaEwgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "sgd_params = {\n",
        "    'loss': ['hinge', 'log'],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "}\n",
        "sgd_clf = SGDClassifier(random_state=202035157)\n",
        "sgd_grid = GridSearchCV(sgd_clf, sgd_params, cv=5)\n",
        "sgd_grid.fit(X_train_processed, y_train)\n",
        "print(\"SGD 최적 파라미터:\", sgd_grid.best_params_)\n",
        "\n",
        "\n",
        "tree_params = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "tree_clf = DecisionTreeClassifier(random_state=202035157)\n",
        "tree_grid = GridSearchCV(tree_clf, tree_params, cv=5)\n",
        "tree_grid.fit(X_train_processed, y_train)\n",
        "print(\"Decision Tree 최적 파라미터:\", tree_grid.best_params_)\n",
        "\n",
        "rf_params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "rf_clf = RandomForestClassifier(random_state=202035157)\n",
        "rf_grid = GridSearchCV(rf_clf, rf_params, cv=5)\n",
        "rf_grid.fit(X_train_processed, y_train)\n",
        "print(\"Random Forest 최적 파라미터:\", rf_grid.best_params_)\n",
        "\n",
        "\n",
        "X_train_dense = X_train_processed.toarray() if hasattr(X_train_processed, 'toarray') else X_train_processed\n",
        "\n",
        "hgb_params = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "hgb_clf = HistGradientBoostingClassifier(random_state=202035157)\n",
        "hgb_grid = GridSearchCV(hgb_clf, hgb_params, cv=5)\n",
        "hgb_grid.fit(X_train_dense, y_train)\n",
        "print(\"HistGradientBoosting 최적 파라미터:\", hgb_grid.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcaWMp1LcfSm",
        "outputId": "cb0521e2-fe44-4932-d21d-11aef77be799"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD 최적 파라미터: {'alpha': 0.001, 'loss': 'hinge', 'penalty': 'l2'}\n",
            "Decision Tree 최적 파라미터: {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 2}\n",
            "Random Forest 최적 파라미터: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "HistGradientBoosting 최적 파라미터: {'learning_rate': 0.01, 'max_depth': None, 'min_samples_leaf': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "이 코드는 SGDClassifier, DecisionTreeClassifier, RandomForestClassifier, HistGradientBoostingClassifier 네 가지 머신 러닝 모델에 대해 하이퍼파라미터 튜닝을 수행합니다. 각 모델에 대해 가능한 하이퍼파라미터의 조합을 탐색하고, 5-폴드 교차 검증을 사용해 각 조합의 성능을 평가합니다. GridSearchCV를 사용하여 최적의 하이퍼파라미터 조합을 찾고, 이를 출력합니다. 여기에 추가로 희소 행렬을 밀집 행렬로 변환하는 작업도 포함되어 있습니다."
      ],
      "metadata": {
        "id": "JaKIwxRUFlwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "optimal_sgd_clf = SGDClassifier(**sgd_grid.best_params_, random_state=202035157)\n",
        "optimal_tree_clf = DecisionTreeClassifier(**tree_grid.best_params_, random_state=202035157)\n",
        "optimal_rf_clf = RandomForestClassifier(**rf_grid.best_params_, random_state=202035157)\n",
        "optimal_hgb_clf = HistGradientBoostingClassifier(**hgb_grid.best_params_, random_state=202035157)\n",
        "\n",
        "\n",
        "X_train_dense = X_train_processed.toarray()\n",
        "X_test_dense = X_test_processed.toarray()\n",
        "\n",
        "optimal_sgd_clf.fit(X_train_processed, y_train)\n",
        "optimal_tree_clf.fit(X_train_processed, y_train)\n",
        "optimal_rf_clf.fit(X_train_processed, y_train)\n",
        "optimal_hgb_clf.fit(X_train_dense, y_train)\n",
        "\n",
        "sgd_preds = optimal_sgd_clf.predict(X_test_processed)\n",
        "tree_preds = optimal_tree_clf.predict(X_test_processed)\n",
        "rf_preds = optimal_rf_clf.predict(X_test_processed)\n",
        "hgb_preds = optimal_hgb_clf.predict(X_test_dense)\n",
        "\n",
        "print(\"SGD Classifier Accuracy: \", accuracy_score(y_test, sgd_preds))\n",
        "print(\"Decision Tree Accuracy: \", accuracy_score(y_test, tree_preds))\n",
        "print(\"Random Forest Accuracy: \", accuracy_score(y_test, rf_preds))\n",
        "print(\"Histogram-based Gradient Boosting Accuracy: \", accuracy_score(y_test, hgb_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F3RUAQ8t7YC",
        "outputId": "626a7a11-af66-47d8-c35b-7b2121f78ec4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD Classifier Accuracy:  0.8212290502793296\n",
            "Decision Tree Accuracy:  0.8044692737430168\n",
            "Random Forest Accuracy:  0.8100558659217877\n",
            "Histogram-based Gradient Boosting Accuracy:  0.776536312849162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "titanic['FamilySize'] = titanic['SibSp'] + titanic['Parch'] + 1\n",
        "titanic['IsAlone'] = titanic['FamilySize'].apply(lambda x: 1 if x == 1 else 0)\n",
        "titanic['AgeGroup'] = pd.cut(titanic['Age'], bins=[0, 18, 35, 60, 100], labels=['Child', 'Young Adult', 'Adult', 'Senior'], right=False)\n",
        "titanic['FareGroup'] = pd.qcut(titanic['Fare'], 4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
        "\n",
        "X = titanic.drop(\"Survived\", axis=1)\n",
        "y = titanic[\"Survived\"]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=202035157)\n",
        "\n",
        "num_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "cat_features = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "\n",
        "num_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', num_transformer, num_features),\n",
        "        ('cat', cat_transformer, cat_features)])\n",
        "\n",
        "\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "sgd_params = {\n",
        "    'loss': ['hinge', 'log'],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "}\n",
        "sgd_clf = SGDClassifier(random_state=202035157)\n",
        "sgd_grid = GridSearchCV(sgd_clf, sgd_params, cv=5)\n",
        "sgd_grid.fit(X_train_processed, y_train)\n",
        "print(\"SGD 최적 파라미터:\", sgd_grid.best_params_)\n",
        "\n",
        "\n",
        "tree_params = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "tree_clf = DecisionTreeClassifier(random_state=202035157)\n",
        "tree_grid = GridSearchCV(tree_clf, tree_params, cv=5)\n",
        "tree_grid.fit(X_train_processed, y_train)\n",
        "print(\"Decision Tree 최적 파라미터:\", tree_grid.best_params_)\n",
        "\n",
        "rf_params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "rf_clf = RandomForestClassifier(random_state=202035157)\n",
        "rf_grid = GridSearchCV(rf_clf, rf_params, cv=5)\n",
        "rf_grid.fit(X_train_processed, y_train)\n",
        "print(\"Random Forest 최적 파라미터:\", rf_grid.best_params_)\n",
        "\n",
        "\n",
        "X_train_dense = X_train_processed.toarray() if hasattr(X_train_processed, 'toarray') else X_train_processed\n",
        "\n",
        "hgb_params = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "hgb_clf = HistGradientBoostingClassifier(random_state=202035157)\n",
        "hgb_grid = GridSearchCV(hgb_clf, hgb_params, cv=5)\n",
        "hgb_grid.fit(X_train_dense, y_train)\n",
        "print(\"HistGradientBoosting 최적 파라미터:\", hgb_grid.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51zLMx_iIewi",
        "outputId": "e07c0db8-a669-461b-9d5a-c00b1f79286d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD 최적 파라미터: {'alpha': 0.0001, 'loss': 'log', 'penalty': 'l2'}\n",
            "Decision Tree 최적 파라미터: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2}\n",
            "Random Forest 최적 파라미터: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "HistGradientBoosting 최적 파라미터: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_leaf': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'SibSp'는 타이타닉에 탑승한 형제, 자매, 배우자의 수를, 'Parch'는 부모와 자녀의 수를 나타냅니다.\n",
        "이 두 특성을 합하여 'FamilySize'라는 새로운 특성을 생성합니다. +1은 본인을 포함하는 것입니다. 'FamilySize'를 기반으로 혼자 여부를 판단하여 'IsAlone'이라는 새로운 이진 특성을 만듭니다. 가족 크기가 1이면 혼자로 판단하고 1을, 그렇지 않으면 0을 할당합니다. 연령을 구분하는 새로운 특성 'AgeGroup'을 만듭니다.\n",
        "pd.cut 함수를 사용하여 'Age' 특성을 여러 그룹으로 나눕니다. 구체적으로는 아이, 젊은 성인, 성인, 그리고 노인으로 구분합니다. 연령을 구분하는 새로운 특성 'AgeGroup'을 만듭니다.\n",
        "pd.cut 함수를 사용하여 'Age' 특성을 여러 그룹으로 나눕니다. 구체적으로는 아이, 젊은 성인, 성인, 그리고 노인으로 구분합니다. 이러한 특성 공학을 통해 기존의 특성을 확장하고, 모델이 데이터에서 더 복잡한 패턴을 학습할 수 있게 돕습니다."
      ],
      "metadata": {
        "id": "8LqnqboEO4BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "optimal_sgd_clf = SGDClassifier(**sgd_grid.best_params_, random_state=202035157)\n",
        "optimal_tree_clf = DecisionTreeClassifier(**tree_grid.best_params_, random_state=202035157)\n",
        "optimal_rf_clf = RandomForestClassifier(**rf_grid.best_params_, random_state=202035157)\n",
        "optimal_hgb_clf = HistGradientBoostingClassifier(**hgb_grid.best_params_, random_state=202035157)\n",
        "\n",
        "\n",
        "X_train_dense = X_train_processed.toarray()\n",
        "X_test_dense = X_test_processed.toarray()\n",
        "\n",
        "optimal_sgd_clf.fit(X_train_processed, y_train)\n",
        "optimal_tree_clf.fit(X_train_processed, y_train)\n",
        "optimal_rf_clf.fit(X_train_processed, y_train)\n",
        "optimal_hgb_clf.fit(X_train_dense, y_train)\n",
        "\n",
        "sgd_preds = optimal_sgd_clf.predict(X_test_processed)\n",
        "tree_preds = optimal_tree_clf.predict(X_test_processed)\n",
        "rf_preds = optimal_rf_clf.predict(X_test_processed)\n",
        "hgb_preds = optimal_hgb_clf.predict(X_test_dense)\n",
        "\n",
        "print(\"SGD Classifier Accuracy: \", accuracy_score(y_test, sgd_preds))\n",
        "print(\"Decision Tree Accuracy: \", accuracy_score(y_test, tree_preds))\n",
        "print(\"Random Forest Accuracy: \", accuracy_score(y_test, rf_preds))\n",
        "print(\"Histogram-based Gradient Boosting Accuracy: \", accuracy_score(y_test, hgb_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8-OmpgwJGHZ",
        "outputId": "46d1a61a-b731-45e5-8ce3-ca37fec9019e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD Classifier Accuracy:  0.8044692737430168\n",
            "Decision Tree Accuracy:  0.7877094972067039\n",
            "Random Forest Accuracy:  0.7988826815642458\n",
            "Histogram-based Gradient Boosting Accuracy:  0.7877094972067039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "특성 공학을 사용한 모델의 Histogram-based Gradient Boosting Accuracy가 이전 모델의 Accuracy보다 더 높은 수치를 도출하였습니다."
      ],
      "metadata": {
        "id": "Y5Z5fqC4PJl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fA_2JNa1OWK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "titanic1 = pd.read_csv(\"/content/drive/MyDrive/machine_dataset/gender_submission.csv\")\n",
        "print(titanic1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mVM-gBwuiz7",
        "outputId": "89efc3c1-4a19-46ae-e553-63edabde923e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     PassengerId  Survived\n",
            "0            892         0\n",
            "1            893         1\n",
            "2            894         0\n",
            "3            895         0\n",
            "4            896         1\n",
            "..           ...       ...\n",
            "413         1305         0\n",
            "414         1306         1\n",
            "415         1307         0\n",
            "416         1308         0\n",
            "417         1309         0\n",
            "\n",
            "[418 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gender_submission 파일 활용"
      ],
      "metadata": {
        "id": "oNC8Q2iBOaB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nan_values = titanic1.isnull().sum()\n",
        "\n",
        "print(nan_values)"
      ],
      "metadata": {
        "id": "hfJPZ8Q9ul07",
        "outputId": "4a06ce48-50d8-4d1f-f5ae-10f73937714a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PassengerId    0\n",
            "Survived       0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "data= titanic1\n",
        "\n",
        "\n",
        "X = data[[\"PassengerId\"]]\n",
        "y = data[\"Survived\"]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=202035157)\n",
        "\n",
        "models = {\n",
        "    \"SGDClassifier\": {\n",
        "        \"model\": SGDClassifier(random_state=202035157),\n",
        "        \"params\": {\n",
        "            'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "            'loss': ['hinge', 'log'],\n",
        "            'penalty': ['l1', 'l2']\n",
        "        }\n",
        "    },\n",
        "    \"DecisionTreeClassifier\": {\n",
        "        \"model\": DecisionTreeClassifier(random_state=202035157),\n",
        "        \"params\": {\n",
        "            'criterion': ['gini', 'entropy'],\n",
        "            'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "            'min_samples_split': [2, 5, 10]\n",
        "        }\n",
        "    },\n",
        "    \"RandomForestClassifier\": {\n",
        "        \"model\": RandomForestClassifier(random_state=202035157),\n",
        "        \"params\": {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [None, 10, 20, 30],\n",
        "            'min_samples_split': [2, 5, 10]\n",
        "        }\n",
        "    },\n",
        "    \"HistGradientBoostingClassifier\": {\n",
        "        \"model\": HistGradientBoostingClassifier(random_state=202035157),\n",
        "        \"params\": {\n",
        "            'max_iter': [100, 200, 300],\n",
        "            'max_depth': [None, 10, 20, 30],\n",
        "            'min_samples_leaf': [5, 10, 20]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "for name, model_info in models.items():\n",
        "    grid_search = GridSearchCV(model_info[\"model\"], model_info[\"params\"], cv=5)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    print(f\"Model: {name}\")\n",
        "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best Cross-Validation Score: {grid_search.best_score_}\")\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    train_score = best_model.score(X_train, y_train)\n",
        "    test_score = best_model.score(X_test, y_test)\n",
        "    print(f\"Training Accuracy: {train_score}\")\n",
        "    print(f\"Test Accuracy: {test_score}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg2jqc6w90mj",
        "outputId": "5c62bd45-6dd7-44af-f9f1-a6dd7ab75ae4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: SGDClassifier\n",
            "Best Parameters: {'alpha': 0.01, 'loss': 'log', 'penalty': 'l1'}\n",
            "Best Cross-Validation Score: 0.5839891451831749\n",
            "Training Accuracy: 0.3592814371257485\n",
            "Test Accuracy: 0.38095238095238093\n",
            "--------------------------------------------------\n",
            "Model: DecisionTreeClassifier\n",
            "Best Parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5}\n",
            "Best Cross-Validation Score: 0.6197648123021258\n",
            "Training Accuracy: 0.6826347305389222\n",
            "Test Accuracy: 0.6309523809523809\n",
            "--------------------------------------------------\n",
            "Model: RandomForestClassifier\n",
            "Best Parameters: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "Best Cross-Validation Score: 0.5537765716870194\n",
            "Training Accuracy: 0.7335329341317365\n",
            "Test Accuracy: 0.5833333333333334\n",
            "--------------------------------------------------\n",
            "Model: HistGradientBoostingClassifier\n",
            "Best Parameters: {'max_depth': None, 'max_iter': 100, 'min_samples_leaf': 20}\n",
            "Best Cross-Validation Score: 0.5625508819538669\n",
            "Training Accuracy: 0.7245508982035929\n",
            "Test Accuracy: 0.5833333333333334\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서는 \"PassengerId\"를 특성으로, \"Survived\"를 타겟 변수로 선택하고 있습니다. test_size=0.2는 테스트 셋이 전체 데이터의 20%를 차지하도록 설정합니다. 20% 선정 이유는 앞서 말씀드린바와 같습니다. SGDClassifier, DecisionTreeClassifier, RandomForestClassifier, HistGradientBoostingClassifier 모델을 딕셔너리에 저장하고 각각에 대한 하이퍼파라미터 그리드를 설정합니다. 각 모델에 대해 GridSearchCV를 수행하여 최적의 하이퍼파라미터를 찾고, 모델을 훈련시킵니다. 마지막으로 각 모델의 최적 하이퍼파라미터, 크로스 밸리데이션 점수, 훈련 정확도, 테스트 정확도를 출력합니다."
      ],
      "metadata": {
        "id": "13JgIn5JGWtH"
      }
    }
  ]
}